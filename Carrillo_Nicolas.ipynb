{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python383jvsc74a57bd0f8af8a5d58c03aea7fed271afda0c18009a73827ffd40ee9cf6d51ffb766ed25",
   "display_name": "Python 3.8.3 64-bit (conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# <center>Tarea 1 Extracción Automática de Información de la web y procesamiento de lenguaje natural</center>\n",
    "### <center> Nicolás Carrillo Sepulveda - Ingenieria civil Telematica </center>\n",
    "### <center> https://github.com/ncarrillo93/tarea1Scapping.git </center>\n",
    "#### <center> 2021</center>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## <center>Pregunta 1</center>\n",
    "#### Elegir un sitio web de noticias distinto a los tratados en clases (tambien distinto al de la/los compañera/os) y extraer, desde algun articulo de noticias en ese sitio web, el titulo y cuerpo de la noticia y, ademas, alguna variable de tipo numerica asociada al articulo. Como ejemplos de esto ultimo, se puede considerar: fecha de la noticia, cantidad de comentarios, cantidad de parrafos, cantidad de links asociados, etc."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Titulo:Scavlab: El experimento de Scavengers que puede tener más de 9000 jugadores al mismo tiempo  \n\n\n\nEditorial: Gaming / Facebook / Twitter / Cobertura / Instagram / Discord\nScavengers es un juego multijugador como muchos otros, su descripción oficial en Steam lo describe como un\"«shooter» estratégico gratuito que mezcla PvE de mundo abierto y PvP de clases\", pero hay algo que hace especial a Scavengers. El juego posee una modalidad multijugador como nunca se ha visto antes, podríamos decir que es un nuevo género dentro de los videojuegos multijugador, este modo experimental desarrollado por Midwinter Entertainment permite crear partidas capaces de albergar más 1700 usuarios en vivo y hasta 9000 jugadores de forma simultánea.\nRecientemente James Davenport, un periodista del medio PC Gamer, tuvo la oportunidad de participar en una sesión de prueba ScavLab, una sesión en la que más de 1700 usuarios se unieron para participar del modo experimental del juego. Este modo en vez de realizar una partida convencional de Scavengers coloca a los jugadores en una especie de evento performativo en el que avatares gigantes guían a los usuarios a través de distintas experiencias sensoriales del juego. Según relata Davenport los administradores, en modo de avatares gigantes que ordenaban el juego, los guiaron por distintas actividades sociales simples, esto como un calentamiento antes de comenzar a jugar tenis. Con los jugadores. Estos avatares están equipados con una capacidad de empuje forzado y envían docenas, a veces cientos de jugadores volando por la escena a la vez.\nPuedes leer: Extraña carta del presidente de Pokémon Company es subastada en cerca de 250 mil dólares\nDavenport escribe: \"Una vez que se detiene el cacareo de los administradores, nos guían a un acantilado. Los meteoritos caen del cielo y se estrellan contra la ladera de la montaña, un administrador amarillo imponente, como, literalmente, del tamaño de una torre, que emerge del impacto de uno. Es Josh Holmes, cofundador y director ejecutivo de Midwinter Entertainment. Habla por un tiempo, agradeciendo a la comunidad antes de anunciar la fecha de lanzamiento del Early Access de Scavengers. Y luego volvemos a soltarnos, esta vez por la ladera de la montaña.\"\n\n\n\"Antes de irnos, los administradores quieren impulsar el entorno de prueba y generar miles de jugadores virtuales más. Si bien no son personas con cuerpo y cerebro, el entorno los registra como jugadores. Logramos llegar a una población total de 9,600 aproximadamente, los administradores se disculparon con los probadores en vivo por cualquier estrés en sus máquinas locales\"\nActualmente Scavenger se encuentra en acceso anticipado de Steam por si deseas descubrir más de este título y a pesar de no contar con este modo de multijugador, si desde Midwinter proyectan incorporarlo a un futuro y seguir experimentando con las posibilidades que esto ofrece tanto a desarrolladores como usuarios.\n\n\nTe recordamos que Tarreo.com también está Instagram y Twitter como @TarreoGamer ¡Síguenos! \n(Fuente)\nEditorial: Gaming / Facebook / Twitter / Cobertura / Instagram / Discord \n\n \n\nla hora en que se publico la noticia fue: 2021-04-27T20:39:00+00:00\n la cantidad de link externo en la pagina son: 24\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url=\"https://www.tarreo.com/noticias/620406/Scavlab-El-experimento-de-Scavengers-que-puede-tener-mas-de-9000-jugadores-al-mismo-tiempo\"\n",
    "html = urlopen(url)\n",
    "bsObj = BeautifulSoup(html.read(),'html.parser')\n",
    "\n",
    "#Titulo\n",
    "titulo= bsObj.find('h1',attrs={'itemprop':'name headline'}).getText()\n",
    "print('Titulo:'+str(titulo))\n",
    "\n",
    "#Contenido\n",
    "content = bsObj.find_all('div',attrs={'itemprop':'articleBody'})\n",
    "for a in content:\n",
    "    print( str(a.getText())+'\\n \\n' )\n",
    "\n",
    "#Datos numericos ( Fecha,**cantidad de links asociados**)\n",
    "    #fecha\n",
    "time = bsObj.find('time').getText()\n",
    "print('la hora en que se publico la noticia fue: '+str(time))\n",
    "    #Cantidad de links asociados\n",
    "links = bsObj.find_all('a',attrs={'target':'_blank'})\n",
    "print(' la cantidad de link externo en la pagina son: '+str(len(links)) )"
   ]
  },
  {
   "source": [
    "## <center>Pregunta 2</center>\n",
    "#### Expandir la extraccion automatizada de la pregunta anterior y programar un web crawler que \n",
    "* identifica donde esta la barra de busqueda en el sitio de noticias elegido, le añade un topico o tema particular \n",
    "* extrae desde todos los articulos encontrados en la primera pagina de resultados de busqueda,\n",
    "    * todos los titulos\n",
    "    * cuerpos de las noticias que se encuentran,\n",
    "    * adem\u0013as de las variables numericas asociadas a cada articulo de noticia."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class Website:\n",
    "    def __init__(self, name, url, searchUrl, resultListing, resultUrl, absoluteUrl):\n",
    "        self.name = name\n",
    "        self.url = url\n",
    "        self.searchUrl = searchUrl \n",
    "        self.resultListing = resultListing\n",
    "        self.resultUrl = resultUrl\n",
    "        self.absoluteUrl=absoluteUrl\n",
    "\n",
    "class Articulo:\n",
    "        def __init__(self,fecha,title,body,numParrafos):\n",
    "            self.fecha = fecha\n",
    "            self.title = title\n",
    "            self.body = body\n",
    "            self.numParrafos = numParrafos\n",
    "\n",
    "class Crawler:\n",
    "\n",
    "    def getPage(self, url):\n",
    "        try:\n",
    "            req = requests.get(url)\n",
    "        except requests.exceptions.RequestException:\n",
    "            return None\n",
    "        return BeautifulSoup(req.text, 'html.parser')  \n",
    "\n",
    "    def getTitle(self,url): #OK\n",
    "        try:\n",
    "            html = urlopen(url)\n",
    "        except HTTPError as e:\n",
    "            return None\n",
    "        try:\n",
    "            bs = BeautifulSoup(html.read(),'lxml')\n",
    "            aux= bs.find('title')\n",
    "            for name in aux:\n",
    "                titles = name\n",
    "        except AttributeError as e:\n",
    "            return None\n",
    "        return titles\n",
    "        \n",
    "    def getBody(self,url):\n",
    "        try:\n",
    "            html = urlopen(url)\n",
    "        except HTTPError as e:\n",
    "            return None\n",
    "        try:\n",
    "            bs = BeautifulSoup(html.read(),'lxml')\n",
    "            aux = bs.find('div',{'class':'main-single-body__content'})\n",
    "            nameList = aux.findAll('p')\n",
    "            noticias=' '\n",
    "            for news in nameList:\n",
    "                noticias= noticias + str(news.getText())\n",
    "        except AttributeError as e:\n",
    "            return None\n",
    "        return noticias\n",
    "\n",
    "    def getNumParrafos(self,url):\n",
    "        try:\n",
    "            html = urlopen(url)\n",
    "        except HTTPError as e:\n",
    "            return None\n",
    "        try:\n",
    "            bs = BeautifulSoup(html.read(),'lxml')\n",
    "            aux = bs.find('div',{'class':'main-single-body__content'})\n",
    "            nameList = aux.findAll('p')\n",
    "        except AttributeError as e:\n",
    "            return None\n",
    "        return len(nameList)\n",
    "\n",
    "    \n",
    "    def getTime(self,url):\n",
    "        try:\n",
    "            html = urlopen(url)\n",
    "        except HTTPError as e:\n",
    "            return None\n",
    "        try:\n",
    "            bs = BeautifulSoup(html.read(),'lxml')\n",
    "            time = bs.find('span',{'class':'main-single-about__item'}).getText()           \n",
    "        except AttributeError as e:\n",
    "            return None\n",
    "        return time\n",
    "\n",
    "    def getInfoArticulos(self,topic,site):\n",
    "\n",
    "        #Obtiene la pagina\n",
    "        bs = self.getPage(site.searchUrl + topic)\n",
    "        #obtiene el div donde esta la información que quiero\n",
    "        searchResults = bs.select(site.resultListing) #selecciona todos los elementos h1 de div.main-content (class=\"main-content\")\n",
    "\n",
    "        listArticulos =[]\n",
    "        articulo = Articulo('Fecha','Titulos','Cuerpo de la noticia','Numero de parrafos')\n",
    "        listArticulos.append(articulo)\n",
    "        #recorre el arreglo para sacar la información de cada noticia\n",
    "        for result in searchResults:\n",
    "            #saca la pagina de la noticia en el buscador\n",
    "            url = result.select(site.resultUrl)[0].attrs['href']\n",
    "            if(site.absoluteUrl):\n",
    "                bs = self.getPage(url)\n",
    "            else:\n",
    "                bs = self.getPage(site.url + url)\n",
    "            \n",
    "            if bs is None:\n",
    "                print(\"El objeto beautifulSoup es None \")\n",
    "                return\n",
    "            title = self.getTitle( url )\n",
    "\n",
    "            if title is None:\n",
    "                print(\"noticia sin titulo o no se encontro\")\n",
    "\n",
    "            fecha = self.getTime(url)\n",
    "\n",
    "            if fecha is None:\n",
    "                print(\"noticia sin titulo o no se encontro\")\n",
    "\n",
    "            body  = self.getBody(url)\n",
    "            numParrafos = 0\n",
    "            if body is None:\n",
    "                print(\"noticia sin cuerpo o no se encontro\")\n",
    "            else:\n",
    "                i=0\n",
    "                for name in body:\n",
    "                    i = i+1\n",
    "            numParrafos = i\n",
    "            listArticulos.append(Articulo(self.getTime(url),self.getTitle(url),self.getBody(url),self.getNumParrafos(url)))\n",
    "        return listArticulos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "crawler = Crawler()\n",
    "list = []\n",
    "#                                          Website(name ,                url         ,           searchUrl              ,    resultListing    , resultUrl , absoluteUrl )\n",
    "list = crawler.getInfoArticulos('Schalper',Website('cnn', 'https://www.cnnchile.com/','https://www.cnnchile.com/search/','div.main-content h2', 'div a'   , True        ))"
   ]
  },
  {
   "source": [
    "## <center>Pregunta 3</center>\n",
    "#### Finalmente, implementar una funcion de Python que permita guardar, en una base de datos ordenada y con formato CSV, la informaci\u0013on extraida. Esto quiere decir que debe considerar guardar cada observacion (titulo del artuculo, cuerpo del articulo, variable numerica) como una fila del CSV."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "#Funciona con la claseArticulo del ejercicio Pasado\n",
    "class tablas:\n",
    "    def crearCSV(self,list):\n",
    "        csvFile=open('tabla.csv','wt+',encoding='utf-8')\n",
    "        writer = csv.writer(csvFile)\n",
    "        listAux=[]\n",
    "        try:\n",
    "            for i in list:\n",
    "                if i.fecha is None:\n",
    "                listAux=[i.fecha , i.title,i.body , i.numParrafos]\n",
    "                writer.writerow(listAux)\n",
    "\n",
    "        finally:\n",
    "            csvFile.close()\n",
    "\n",
    "#test\n",
    "miTabla = tablas()\n",
    "miTabla.crearCSV(list)\n",
    "import pandas as pd \n",
    "df = pd.read_csv('tabla.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}